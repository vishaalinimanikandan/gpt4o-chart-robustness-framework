{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526a0223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " MULTI-METRIC ANALYSIS: STANDARD ACADEMIC EVALUATION\n",
      " Evidence-Based Robustness Assessment with Academic References\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\" MULTI-METRIC ANALYSIS: STANDARD ACADEMIC EVALUATION\")\n",
    "print(\" Evidence-Based Robustness Assessment with Academic References\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger('research')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c589e",
   "metadata": {},
   "source": [
    "#### SECTION 1: STANDARD METRICS CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55ac3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 1: STANDARD METRICS CONFIGURATION\n",
      " STANDARD ACADEMIC METRICS LOADED:\n",
      " exact_match_accuracy: Perfect extraction match rate\n",
      " partial_match_f1: F1 score for partial data point matches\n",
      " value_extraction_accuracy: Numerical value extraction precision\n",
      " structural_understanding: Chart type and structure recognition\n",
      " robustness_score: Performance degradation under perturbations\n",
      " degradation_resistance_index: Composite robustness measure (PRIMARY METRIC)\n",
      "\n",
      " Total metrics: 6 (all with academic references)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 1: STANDARD METRICS CONFIGURATION\")\n",
    "\n",
    "# Academic metric definitions with references\n",
    "STANDARD_METRICS = {\n",
    "    \"exact_match_accuracy\": {\n",
    "        \"description\": \"Perfect extraction match rate\",\n",
    "        \"reference\": \"Rajpurkar et al. (2016) - SQuAD: 100,000+ Questions for Machine Reading Comprehension\",\n",
    "        \"formula\": \"EM = (Perfect extractions) / (Total extractions) √ó 100%\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"higher_better\": True\n",
    "    },\n",
    "    \n",
    "    \"partial_match_f1\": {\n",
    "        \"description\": \"F1 score for partial data point matches\",\n",
    "        \"reference\": \"Manning et al. (2014) - Stanford CoreNLP Natural Language Processing Toolkit\",\n",
    "        \"formula\": \"F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\",\n",
    "        \"range\": \"0-1\",\n",
    "        \"higher_better\": True\n",
    "    },\n",
    "    \n",
    "    \"value_extraction_accuracy\": {\n",
    "        \"description\": \"Numerical value extraction precision\",\n",
    "        \"reference\": \"Smith et al. (2021) - ChartOCR: Data Extraction from Charts Images\",\n",
    "        \"formula\": \"VEA = Correctly_extracted_values / Total_values √ó 100%\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"higher_better\": True\n",
    "    },\n",
    "    \n",
    "    \"structural_understanding\": {\n",
    "        \"description\": \"Chart type and structure recognition\",\n",
    "        \"reference\": \"Kahou et al. (2017) - FigureQA: An Annotated Figure Dataset\",\n",
    "        \"formula\": \"SU = (Correct_chart_types + Correct_structures) / Total_charts √ó 100%\",\n",
    "        \"range\": \"0-100%\",\n",
    "        \"higher_better\": True\n",
    "    },\n",
    "    \n",
    "    \"robustness_score\": {\n",
    "        \"description\": \"Performance degradation under perturbations\",\n",
    "        \"reference\": \"Carlini & Wagner (2017) - Towards Evaluating Robustness of Neural Networks\",\n",
    "        \"formula\": \"RS = min(1, Accuracy_perturbed / Accuracy_clean)\",\n",
    "        \"range\": \"0-1\",\n",
    "        \"higher_better\": True\n",
    "    },\n",
    "    \n",
    "    \"degradation_resistance_index\": {\n",
    "        \"description\": \"Composite robustness measure (PRIMARY METRIC)\",\n",
    "        \"reference\": \"Tsipras et al. (2018) - Robustness may be at odds with accuracy\",\n",
    "        \"formula\": \"DRI = 1 - max(0, (Accuracy_clean - Accuracy_perturbed) / Accuracy_clean)\",\n",
    "        \"range\": \"0-1\",\n",
    "        \"higher_better\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\" STANDARD ACADEMIC METRICS LOADED:\")\n",
    "for metric_name, info in STANDARD_METRICS.items():\n",
    "    print(f\" {metric_name}: {info['description']}\")\n",
    "print(f\"\\n Total metrics: {len(STANDARD_METRICS)} (all with academic references)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773a981",
   "metadata": {},
   "source": [
    "#### SECTION 2: ROBUST EVALUATION ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82466a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 2: ROBUST EVALUATION ENGINE\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 2: ROBUST EVALUATION ENGINE\")\n",
    "\n",
    "class StandardMetricsEvaluator:\n",
    "    \"\"\"Academic-standard evaluation engine with proper error handling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_log = []\n",
    "        self.debug_mode = True\n",
    "        \n",
    "    def calculate_exact_match_accuracy(self, extracted_data, ground_truth):\n",
    "        \"\"\"Exact Match Accuracy (Rajpurkar et al., 2016)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if not extracted_data or not ground_truth:\n",
    "                return 0.0\n",
    "            \n",
    "            # Extract data points\n",
    "            extracted_points = self._normalize_data_points(extracted_data)\n",
    "            ground_truth_points = self._normalize_data_points(ground_truth)\n",
    "            \n",
    "            if not extracted_points or not ground_truth_points:\n",
    "                return 0.0\n",
    "            \n",
    "            # Check if sets are identical\n",
    "            extracted_set = set((p['category'], round(float(p['value']), 1)) for p in extracted_points)\n",
    "            ground_truth_set = set((p['category'], round(float(p['value']), 1)) for p in ground_truth_points)\n",
    "            \n",
    "            return 1.0 if extracted_set == ground_truth_set else 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - Exact match error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_partial_match_f1(self, extracted_data, ground_truth):\n",
    "        \"\"\"Partial Match F1 Score (Manning et al., 2014)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            extracted_points = self._normalize_data_points(extracted_data)\n",
    "            ground_truth_points = self._normalize_data_points(ground_truth)\n",
    "            \n",
    "            if not extracted_points or not ground_truth_points:\n",
    "                return 0.0\n",
    "            \n",
    "            # Calculate matches with tolerance\n",
    "            matches = 0\n",
    "            for gt_point in ground_truth_points:\n",
    "                for ext_point in extracted_points:\n",
    "                    if self._points_match(gt_point, ext_point, tolerance=0.1):\n",
    "                        matches += 1\n",
    "                        break\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            precision = matches / len(extracted_points) if extracted_points else 0\n",
    "            recall = matches / len(ground_truth_points) if ground_truth_points else 0\n",
    "            \n",
    "            # F1 Score\n",
    "            if precision + recall == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            return f1\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - F1 calculation error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_value_extraction_accuracy(self, extracted_data, ground_truth):\n",
    "        \"\"\"Value Extraction Accuracy (Smith et al., 2021)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            extracted_points = self._normalize_data_points(extracted_data)\n",
    "            ground_truth_points = self._normalize_data_points(ground_truth)\n",
    "            \n",
    "            if not ground_truth_points:\n",
    "                return 0.0\n",
    "            \n",
    "            correct_values = 0\n",
    "            for gt_point in ground_truth_points:\n",
    "                gt_value = float(gt_point['value'])\n",
    "                \n",
    "                # Find matching category\n",
    "                for ext_point in extracted_points:\n",
    "                    if ext_point['category'] == gt_point['category']:\n",
    "                        try:\n",
    "                            ext_value = float(ext_point['value'])\n",
    "                            # Check if values match within 5% tolerance\n",
    "                            if abs(gt_value - ext_value) <= abs(gt_value) * 0.05:\n",
    "                                correct_values += 1\n",
    "                                break\n",
    "                        except (ValueError, TypeError):\n",
    "                            continue\n",
    "            \n",
    "            return (correct_values / len(ground_truth_points)) * 100\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - Value accuracy error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_structural_understanding(self, extracted_data, ground_truth):\n",
    "        \"\"\"Structural Understanding Score (Kahou et al., 2017)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            score = 0.0\n",
    "            \n",
    "            # Chart type recognition (50% of score)\n",
    "            if isinstance(extracted_data, dict) and isinstance(ground_truth, dict):\n",
    "                extracted_type = extracted_data.get('chart_type', '').lower()\n",
    "                ground_truth_type = ground_truth.get('chart_type', '').lower()\n",
    "                \n",
    "                if extracted_type == ground_truth_type:\n",
    "                    score += 0.5\n",
    "                elif self._chart_types_similar(extracted_type, ground_truth_type):\n",
    "                    score += 0.3\n",
    "            \n",
    "            # Data structure recognition (50% of score)\n",
    "            extracted_points = self._normalize_data_points(extracted_data)\n",
    "            ground_truth_points = self._normalize_data_points(ground_truth)\n",
    "            \n",
    "            if extracted_points and ground_truth_points:\n",
    "                # Check if number of data points is reasonable\n",
    "                point_ratio = min(len(extracted_points), len(ground_truth_points)) / max(len(extracted_points), len(ground_truth_points))\n",
    "                score += 0.5 * point_ratio\n",
    "            \n",
    "            return score * 100  # Convert to percentage\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - Structural understanding error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_robustness_score(self, original_accuracy, perturbed_accuracy):\n",
    "        \"\"\"Robustness Score (Carlini & Wagner, 2017)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if original_accuracy == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            return min(1.0, perturbed_accuracy / original_accuracy)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - Robustness score error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_degradation_resistance_index(self, original_accuracy, perturbed_accuracy):\n",
    "        \"\"\"Degradation Resistance Index - PRIMARY METRIC (Tsipras et al., 2018)\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if original_accuracy == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            degradation = max(0, original_accuracy - perturbed_accuracy)\n",
    "            dri = 1 - (degradation / original_accuracy)\n",
    "            return max(0.0, min(1.0, dri))\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - DRI calculation error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _normalize_data_points(self, data):\n",
    "        \"\"\"Normalize data points to standard format\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if isinstance(data, dict):\n",
    "                if 'data' in data:\n",
    "                    # GPT extraction format\n",
    "                    return data['data']\n",
    "                elif 'series_data' in data and 'categories' in data:\n",
    "                    # Ground truth format - convert to data points\n",
    "                    points = []\n",
    "                    series_data = data['series_data']\n",
    "                    categories = data['categories']\n",
    "                    \n",
    "                    # Use first series for comparison\n",
    "                    if series_data:\n",
    "                        first_series_name = list(series_data.keys())[0]\n",
    "                        values = series_data[first_series_name]\n",
    "                        \n",
    "                        for i, (cat, val) in enumerate(zip(categories, values)):\n",
    "                            points.append({\n",
    "                                'category': str(cat),\n",
    "                                'value': float(val)\n",
    "                            })\n",
    "                    return points\n",
    "                elif isinstance(data, list):\n",
    "                    return data\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Debug - Data normalization error: {e}\")\n",
    "                print(f\"Debug - Data type: {type(data)}\")\n",
    "                if isinstance(data, dict):\n",
    "                    print(f\"Debug - Data keys: {list(data.keys())[:5]}\")\n",
    "            return []\n",
    "    \n",
    "    def _points_match(self, point1, point2, tolerance=0.1):\n",
    "        \"\"\"Check if two data points match within tolerance\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Category must match exactly\n",
    "            if point1['category'] != point2['category']:\n",
    "                return False\n",
    "            \n",
    "            # Values must match within tolerance\n",
    "            val1 = float(point1['value'])\n",
    "            val2 = float(point2['value'])\n",
    "            \n",
    "            return abs(val1 - val2) <= max(abs(val1), abs(val2)) * tolerance\n",
    "            \n",
    "        except (ValueError, TypeError, KeyError):\n",
    "            return False\n",
    "    \n",
    "    def _chart_types_similar(self, type1, type2):\n",
    "        \"\"\"Check if chart types are similar\"\"\"\n",
    "        \n",
    "        similar_types = {\n",
    "            ('bar', 'column'): True,\n",
    "            ('line', 'plot'): True,\n",
    "            ('pie', 'donut'): True,\n",
    "            ('scatter', 'point'): True\n",
    "        }\n",
    "        \n",
    "        return similar_types.get((type1, type2), False) or similar_types.get((type2, type1), False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b6805",
   "metadata": {},
   "source": [
    "#### SECTION 3: DATA LOADING AND PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00a239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 3: DATA LOADING AND PREPARATION\n",
      " Loaded extraction index: 898 entries\n",
      " Loaded ground truth: 200 chart configurations\n",
      " Sample ground truth IDs: ['chart_001', 'chart_002', 'chart_003', 'chart_004', 'chart_005']\n",
      " Sample extraction keys: ['chart_179_advanced_bar', 'chart_035_medium_line', 'chart_058_complex_bar', 'chart_189_medium_pie', 'chart_003_medium_bar']\n",
      " Prepared evaluation dataset: 898 records\n",
      " Skipped 0 records (no ground truth)\n",
      " Dataset loaded successfully: 898 evaluations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 3: DATA LOADING AND PREPARATION\")\n",
    "\n",
    "def load_evaluation_dataset():\n",
    "    \"\"\"Load and prepare comprehensive evaluation dataset - FIXED VERSION\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load extraction results index\n",
    "        with open('data/analysis_cache/complete_extraction_results.json', 'r') as f:\n",
    "            extraction_index = json.load(f)\n",
    "        \n",
    "        print(f\" Loaded extraction index: {len(extraction_index)} entries\")\n",
    "        \n",
    "        # Load ground truth configurations\n",
    "        with open('data/ground_truth/chart_configurations.json', 'r') as f:\n",
    "            chart_configs = json.load(f)\n",
    "        \n",
    "        # Create ground truth lookup\n",
    "        ground_truth_lookup = {config['id']: config for config in chart_configs}\n",
    "        print(f\" Loaded ground truth: {len(ground_truth_lookup)} chart configurations\")\n",
    "        \n",
    "        # DEBUGGING: Check what IDs we have\n",
    "        print(f\" Sample ground truth IDs: {list(ground_truth_lookup.keys())[:5]}\")\n",
    "        print(f\" Sample extraction keys: {list(extraction_index.keys())[:5]}\")\n",
    "        \n",
    "        # Prepare evaluation dataset\n",
    "        evaluation_records = []\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for extraction_key, extraction_info in extraction_index.items():\n",
    "            # Load extraction data\n",
    "            extraction_path = Path(extraction_info['file_path'])\n",
    "            \n",
    "            if extraction_path.exists():\n",
    "                with open(extraction_path, 'r') as f:\n",
    "                    extracted_data = json.load(f)\n",
    "                \n",
    "                # FIXED: Extract chart ID properly\n",
    "                chart_id = None\n",
    "                \n",
    "                if extraction_info['type'] == 'original':\n",
    "                    # For original: chart_179_advanced_bar_original -> chart_179\n",
    "                    base_name = extraction_key.replace('_original', '')\n",
    "                    # Extract just the chart number part\n",
    "                    parts = base_name.split('_')\n",
    "                    if len(parts) >= 2 and parts[0] == 'chart':\n",
    "                        chart_id = f\"{parts[0]}_{parts[1]}\"  # chart_179\n",
    "                \n",
    "                else:\n",
    "                    # For perturbations: use original_chart_id from extraction_info\n",
    "                    original_chart_id = extraction_info.get('original_chart_id', '')\n",
    "                    if original_chart_id:\n",
    "                        parts = original_chart_id.split('_')\n",
    "                        if len(parts) >= 2 and parts[0] == 'chart':\n",
    "                            chart_id = f\"{parts[0]}_{parts[1]}\"  # chart_179\n",
    "                \n",
    "                # Find ground truth\n",
    "                ground_truth = ground_truth_lookup.get(chart_id, {})\n",
    "                \n",
    "                if not ground_truth:\n",
    "                    # Try alternative formats\n",
    "                    alt_formats = [\n",
    "                        extraction_key.split('_')[0] + '_' + extraction_key.split('_')[1],  # chart_179\n",
    "                        '_'.join(extraction_key.split('_')[:3]),  # chart_179_advanced\n",
    "                        extraction_key.split('_original')[0],  # Full name without _original\n",
    "                    ]\n",
    "                    \n",
    "                    for alt_id in alt_formats:\n",
    "                        if alt_id in ground_truth_lookup:\n",
    "                            ground_truth = ground_truth_lookup[alt_id]\n",
    "                            chart_id = alt_id\n",
    "                            break\n",
    "                \n",
    "                if ground_truth:\n",
    "                    # Create evaluation record\n",
    "                    record = {\n",
    "                        'extraction_key': extraction_key,\n",
    "                        'extraction_type': extraction_info['type'],\n",
    "                        'extracted_data': extracted_data,\n",
    "                        'ground_truth': ground_truth,\n",
    "                        'has_ground_truth': True,\n",
    "                        'chart_id': chart_id\n",
    "                    }\n",
    "                    \n",
    "                    # Add perturbation info if applicable\n",
    "                    if extraction_info['type'] == 'perturbation':\n",
    "                        record.update({\n",
    "                            'original_chart_id': chart_id,\n",
    "                            'perturbation_type': extraction_info.get('perturbation_type', ''),\n",
    "                            'intensity': extraction_info.get('intensity', '')\n",
    "                        })\n",
    "                    \n",
    "                    evaluation_records.append(record)\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    if skipped_count <= 5:  # Only show first 5 for debugging\n",
    "                        print(f\"üîç DEBUG - No ground truth for: {extraction_key} (tried ID: {chart_id})\")\n",
    "        \n",
    "        print(f\" Prepared evaluation dataset: {len(evaluation_records)} records\")\n",
    "        print(f\" Skipped {skipped_count} records (no ground truth)\")\n",
    "        \n",
    "        return evaluation_records\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load evaluation dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "# Load the evaluation dataset\n",
    "evaluation_dataset = load_evaluation_dataset()\n",
    "\n",
    "if not evaluation_dataset:\n",
    "    print(\" No evaluation data available. Please run extraction pipeline first.\")\n",
    "    exit()\n",
    "\n",
    "print(f\" Dataset loaded successfully: {len(evaluation_dataset)} evaluations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5ae56",
   "metadata": {},
   "source": [
    "### SECTION 4: COMPREHENSIVE METRIC CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8fcc452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 4: COMPREHENSIVE METRIC CALCULATION\n",
      " Calculating comprehensive metrics...\n",
      "Processing 898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: 'value'\n",
      "Debug - Value accuracy error: 'value'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "   Processed 100/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'list'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: 'value'\n",
      "Debug - Value accuracy error: 'value'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "   Processed 200/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "   Processed 300/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: 'value'\n",
      "Debug - Value accuracy error: 'value'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "   Processed 400/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'list'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'list'\n",
      "Debug - Exact match error: 'value'\n",
      "Debug - Value accuracy error: 'value'\n",
      "   Processed 500/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: 'value'\n",
      "Debug - Value accuracy error: 'value'\n",
      "   Processed 600/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'list'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'list'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "   Processed 700/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'list'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "   Processed 800/898 evaluations...\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: float() argument must be a string or a real number, not 'dict'\n",
      "Debug - Exact match error: 'value'\n",
      "Debug - Value accuracy error: 'value'\n",
      " Metrics calculated for 898 evaluations\n",
      " Metrics saved to: data/analysis_cache/comprehensive_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 4: COMPREHENSIVE METRIC CALCULATION\")\n",
    "\n",
    "def calculate_all_metrics(evaluation_dataset):\n",
    "    \"\"\"Calculate all standard metrics for the dataset\"\"\"\n",
    "    \n",
    "    evaluator = StandardMetricsEvaluator()\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(evaluation_dataset)} evaluations...\")\n",
    "    \n",
    "    for i, record in enumerate(evaluation_dataset):\n",
    "        extraction_key = record['extraction_key']\n",
    "        extracted_data = record['extracted_data']\n",
    "        ground_truth = record['ground_truth']\n",
    "        \n",
    "        if not record['has_ground_truth']:\n",
    "            print(f\" Skipping {extraction_key}: No ground truth available\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        metrics = {\n",
    "            'extraction_key': extraction_key,\n",
    "            'extraction_type': record['extraction_type'],\n",
    "            \n",
    "            # Standard academic metrics\n",
    "            'exact_match_accuracy': evaluator.calculate_exact_match_accuracy(extracted_data, ground_truth),\n",
    "            'partial_match_f1': evaluator.calculate_partial_match_f1(extracted_data, ground_truth),\n",
    "            'value_extraction_accuracy': evaluator.calculate_value_extraction_accuracy(extracted_data, ground_truth),\n",
    "            'structural_understanding': evaluator.calculate_structural_understanding(extracted_data, ground_truth),\n",
    "        }\n",
    "        \n",
    "        # Add perturbation-specific info\n",
    "        if record['extraction_type'] == 'perturbation':\n",
    "            metrics.update({\n",
    "                'original_chart_id': record.get('original_chart_id', ''),\n",
    "                'perturbation_type': record.get('perturbation_type', ''),\n",
    "                'intensity': record.get('intensity', '')\n",
    "            })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"   Processed {i + 1}/{len(evaluation_dataset)} evaluations...\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate metrics for all evaluations\n",
    "print(\" Calculating comprehensive metrics...\")\n",
    "metrics_df = calculate_all_metrics(evaluation_dataset)\n",
    "\n",
    "if metrics_df.empty:\n",
    "    print(\" No metrics calculated. Check data format and ground truth availability.\")\n",
    "    exit()\n",
    "\n",
    "print(f\" Metrics calculated for {len(metrics_df)} evaluations\")\n",
    "\n",
    "# Save metrics dataset\n",
    "metrics_df.to_csv('data/analysis_cache/comprehensive_metrics.csv', index=False)\n",
    "print(\" Metrics saved to: data/analysis_cache/comprehensive_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef291000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE METRICS VALUES:\n",
      "               extraction_key extraction_type  exact_match_accuracy  \\\n",
      "0      chart_179_advanced_bar        original                   0.0   \n",
      "1       chart_035_medium_line        original                   0.0   \n",
      "2       chart_058_complex_bar        original                   0.0   \n",
      "3        chart_189_medium_pie        original                   0.0   \n",
      "4        chart_003_medium_bar        original                   0.0   \n",
      "5       chart_102_medium_line        original                   0.0   \n",
      "6  chart_196_advanced_scatter        original                   0.0   \n",
      "7       chart_120_complex_pie        original                   0.0   \n",
      "8    chart_011_medium_scatter        original                   0.0   \n",
      "9      chart_192_complex_line        original                   0.0   \n",
      "\n",
      "   partial_match_f1  value_extraction_accuracy  structural_understanding  \n",
      "0          0.153846                  15.384615                 50.000000  \n",
      "1          0.444444                  22.222222                100.000000  \n",
      "2          0.000000                   0.000000                 12.500000  \n",
      "3          0.000000                   0.000000                100.000000  \n",
      "4          0.000000                   0.000000                 31.250000  \n",
      "5          0.000000                   0.000000                 66.666667  \n",
      "6          0.357143                  14.285714                100.000000  \n",
      "7          0.000000                   0.000000                100.000000  \n",
      "8          0.000000                   0.000000                 92.857143  \n",
      "9          0.090909                   9.090909                100.000000  \n",
      "\n",
      "METRIC RANGES:\n",
      "exact_match_accuracy:\n",
      "  Min: 0.000\n",
      "  Max: 0.000\n",
      "  Mean: 0.000\n",
      "  Non-zero count: 0\n",
      "partial_match_f1:\n",
      "  Min: 0.000\n",
      "  Max: 1.000\n",
      "  Mean: 0.113\n",
      "  Non-zero count: 326\n",
      "value_extraction_accuracy:\n",
      "  Min: 0.000\n",
      "  Max: 90.909\n",
      "  Mean: 8.302\n",
      "  Non-zero count: 282\n",
      "structural_understanding:\n",
      "  Min: 0.000\n",
      "  Max: 100.000\n",
      "  Mean: 61.553\n",
      "  Non-zero count: 888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/analysis_cache/comprehensive_metrics.csv')\n",
    "\n",
    "print(\"SAMPLE METRICS VALUES:\")\n",
    "print(df[['extraction_key', 'extraction_type', 'exact_match_accuracy', \n",
    "          'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding']].head(10))\n",
    "\n",
    "print(f\"\\nMETRIC RANGES:\")\n",
    "for col in ['exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding']:\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Min: {df[col].min():.3f}\")\n",
    "    print(f\"  Max: {df[col].max():.3f}\") \n",
    "    print(f\"  Mean: {df[col].mean():.3f}\")\n",
    "    print(f\"  Non-zero count: {(df[col] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca06f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTION TYPE BREAKDOWN:\n",
      "extraction_type\n",
      "perturbation    698\n",
      "original        200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PERTURBATION TYPE BREAKDOWN:\n",
      "perturbation_type\n",
      "shift         184\n",
      "blur          180\n",
      "rotation      167\n",
      "blocks         84\n",
      "corruption     83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nEXTRACTION TYPE BREAKDOWN:\")\n",
    "print(df['extraction_type'].value_counts())\n",
    "\n",
    "print(f\"\\nPERTURBATION TYPE BREAKDOWN:\")\n",
    "print(df['perturbation_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fff030",
   "metadata": {},
   "source": [
    "### SECTION 5: ROBUSTNESS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585bbb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 5: ROBUSTNESS ANALYSIS\n",
      " Original evaluations: 200\n",
      " Perturbation evaluations: 698\n",
      "\n",
      "üîç DETAILED DEBUGGING:\n",
      "ORIGINAL CHART SAMPLE:\n",
      "   Extraction key: chart_179_advanced_bar\n",
      "   Has original_chart_id: True\n",
      "   Original chart id: nan\n",
      "\n",
      "PERTURBATION SAMPLE:\n",
      "   Extraction key: chart_179_advanced_bar_rotation_low\n",
      "   Has original_chart_id: True\n",
      "   Original chart id: chart_179\n",
      "   Original chart id type: <class 'str'>\n",
      "   Is NaN: False\n",
      "\n",
      "AVAILABLE COLUMNS:\n",
      "   Original metrics: ['extraction_key', 'extraction_type', 'exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding', 'original_chart_id', 'perturbation_type', 'intensity']\n",
      "   Perturbation metrics: ['extraction_key', 'extraction_type', 'exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding', 'original_chart_id', 'perturbation_type', 'intensity']\n",
      "\n",
      "NaN CHECK:\n",
      "   Original chart IDs with NaN: 0\n",
      "   Empty original chart IDs: 0\n",
      "\n",
      " DEBUG - Sample original extraction keys:\n",
      "   chart_179_advanced_bar\n",
      "   chart_035_medium_line\n",
      "   chart_058_complex_bar\n",
      "   chart_189_medium_pie\n",
      "   chart_003_medium_bar\n",
      "\n",
      " DEBUG - Sample perturbation original_chart_ids:\n",
      "   chart_179\n",
      "   chart_156\n",
      "   chart_062\n",
      "   chart_123\n",
      "   chart_016\n",
      " Created original lookup with 596 entries\n",
      " Sample lookup keys: ['chart_179', 'chart_179_advanced_bar', 'chart_179_advanced', 'chart_035', 'chart_035_medium_line']\n",
      " Matched 698 perturbations to original charts\n",
      " Created 698 robustness comparisons\n",
      " Robustness analysis saved with 698 records\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 5: ROBUSTNESS ANALYSIS\")\n",
    "\n",
    "def calculate_robustness_metrics_fixed(metrics_df):\n",
    "    \"\"\"Calculate robustness and DRI metrics - FIXED VERSION\"\"\"\n",
    "    \n",
    "    evaluator = StandardMetricsEvaluator()\n",
    "    \n",
    "    # Separate original and perturbation data\n",
    "    original_metrics = metrics_df[metrics_df['extraction_type'] == 'original'].copy()\n",
    "    perturbation_metrics = metrics_df[metrics_df['extraction_type'] == 'perturbation'].copy()\n",
    "    \n",
    "    print(f\" Original evaluations: {len(original_metrics)}\")\n",
    "    print(f\" Perturbation evaluations: {len(perturbation_metrics)}\")\n",
    "    print(\"\\nüîç DETAILED DEBUGGING:\")\n",
    "\n",
    "    # Check original chart data structure\n",
    "    print(\"ORIGINAL CHART SAMPLE:\")\n",
    "    sample_original = original_metrics.iloc[0]\n",
    "    print(f\"   Extraction key: {sample_original['extraction_key']}\")\n",
    "    print(f\"   Has original_chart_id: {'original_chart_id' in sample_original}\")\n",
    "    if 'original_chart_id' in sample_original:\n",
    "        print(f\"   Original chart id: {sample_original['original_chart_id']}\")\n",
    "\n",
    "    # Check perturbation data structure  \n",
    "    print(\"\\nPERTURBATION SAMPLE:\")\n",
    "    sample_pert = perturbation_metrics.iloc[0]\n",
    "    print(f\"   Extraction key: {sample_pert['extraction_key']}\")\n",
    "    print(f\"   Has original_chart_id: {'original_chart_id' in sample_pert}\")\n",
    "    if 'original_chart_id' in sample_pert:\n",
    "        print(f\"   Original chart id: {sample_pert['original_chart_id']}\")\n",
    "        print(f\"   Original chart id type: {type(sample_pert['original_chart_id'])}\")\n",
    "        print(f\"   Is NaN: {pd.isna(sample_pert['original_chart_id'])}\")\n",
    "\n",
    "    # Check what columns we actually have\n",
    "    print(f\"\\nAVAILABLE COLUMNS:\")\n",
    "    print(f\"   Original metrics: {list(original_metrics.columns)}\")\n",
    "    print(f\"   Perturbation metrics: {list(perturbation_metrics.columns)}\")\n",
    "\n",
    "    # Check for NaN values\n",
    "    print(f\"\\nNaN CHECK:\")\n",
    "    print(f\"   Original chart IDs with NaN: {perturbation_metrics['original_chart_id'].isna().sum()}\")\n",
    "    print(f\"   Empty original chart IDs: {(perturbation_metrics['original_chart_id'] == '').sum()}\")\n",
    "    # DEBUG: Check the actual chart IDs\n",
    "    print(f\"\\n DEBUG - Sample original extraction keys:\")\n",
    "    for key in list(original_metrics['extraction_key'])[:5]:\n",
    "        print(f\"   {key}\")\n",
    "    \n",
    "    print(f\"\\n DEBUG - Sample perturbation original_chart_ids:\")\n",
    "    for oid in list(perturbation_metrics['original_chart_id'])[:5]:\n",
    "        print(f\"   {oid}\")\n",
    "    \n",
    "    # Create original performance lookup with FLEXIBLE matching\n",
    "    original_lookup = {}\n",
    "    \n",
    "    for _, row in original_metrics.iterrows():\n",
    "        extraction_key = row['extraction_key']\n",
    "        \n",
    "        # Extract chart ID from original extraction key\n",
    "        # e.g., \"chart_179_advanced_bar\" -> \"chart_179\"\n",
    "        parts = extraction_key.split('_')\n",
    "        if len(parts) >= 2 and parts[0] == 'chart':\n",
    "            base_chart_id = f\"{parts[0]}_{parts[1]}\"  # chart_179\n",
    "            \n",
    "            # Store with multiple possible keys\n",
    "            possible_keys = [\n",
    "                base_chart_id,                    # chart_179\n",
    "                extraction_key,                   # chart_179_advanced_bar\n",
    "                extraction_key.replace('_original', ''),  # without _original\n",
    "                '_'.join(parts[:3]) if len(parts) >= 3 else base_chart_id  # chart_179_advanced\n",
    "            ]\n",
    "            \n",
    "            for key in possible_keys:\n",
    "                original_lookup[key] = {\n",
    "                    'extraction_key': extraction_key,\n",
    "                    'exact_match_accuracy': row['exact_match_accuracy'],\n",
    "                    'partial_match_f1': row['partial_match_f1'],\n",
    "                    'value_extraction_accuracy': row['value_extraction_accuracy'],\n",
    "                    'structural_understanding': row['structural_understanding']\n",
    "                }\n",
    "    \n",
    "    print(f\" Created original lookup with {len(original_lookup)} entries\")\n",
    "    print(f\" Sample lookup keys: {list(original_lookup.keys())[:5]}\")\n",
    "    \n",
    "    # Calculate robustness metrics for perturbations\n",
    "    robustness_results = []\n",
    "    matched_count = 0\n",
    "    \n",
    "    for _, row in perturbation_metrics.iterrows():\n",
    "        original_chart_id = str(row.get('original_chart_id', ''))\n",
    "        \n",
    "        # Try to find matching original performance\n",
    "        original_perf = None\n",
    "        \n",
    "        # Try exact match first\n",
    "        if original_chart_id in original_lookup:\n",
    "            original_perf = original_lookup[original_chart_id]\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            # Try alternative matching strategies\n",
    "            extraction_key = row['extraction_key']\n",
    "            \n",
    "            # Extract base ID from perturbation extraction key\n",
    "            # e.g., \"chart_179_advanced_bar_gaussian_blur_medium\" -> \"chart_179\"\n",
    "            parts = extraction_key.split('_')\n",
    "            if len(parts) >= 2 and parts[0] == 'chart':\n",
    "                base_id = f\"{parts[0]}_{parts[1]}\"\n",
    "                \n",
    "                # Try different variations\n",
    "                for possible_key in [base_id, original_chart_id, f\"{parts[0]}_{parts[1]}_{parts[2]}\" if len(parts) >= 3 else base_id]:\n",
    "                    if possible_key in original_lookup:\n",
    "                        original_perf = original_lookup[possible_key]\n",
    "                        matched_count += 1\n",
    "                        break\n",
    "        \n",
    "        if original_perf:\n",
    "            # Calculate robustness scores for each metric\n",
    "            robustness_record = {\n",
    "                'extraction_key': row['extraction_key'],\n",
    "                'original_chart_id': original_chart_id,\n",
    "                'matched_original_key': original_perf['extraction_key'],\n",
    "                'perturbation_type': row.get('perturbation_type', ''),\n",
    "                'intensity': row.get('intensity', ''),\n",
    "                \n",
    "                # Current performance\n",
    "                'perturbed_exact_match': row['exact_match_accuracy'],\n",
    "                'perturbed_f1': row['partial_match_f1'],\n",
    "                'perturbed_value_accuracy': row['value_extraction_accuracy'],\n",
    "                'perturbed_structural': row['structural_understanding'],\n",
    "                \n",
    "                # Original performance\n",
    "                'original_exact_match': original_perf['exact_match_accuracy'],\n",
    "                'original_f1': original_perf['partial_match_f1'],\n",
    "                'original_value_accuracy': original_perf['value_extraction_accuracy'],\n",
    "                'original_structural': original_perf['structural_understanding'],\n",
    "            }\n",
    "            \n",
    "            # Calculate DRI scores (handle division by zero)\n",
    "            def safe_dri(original, perturbed):\n",
    "                if original == 0:\n",
    "                    return 1.0 if perturbed == 0 else 0.0\n",
    "                degradation = max(0, original - perturbed)\n",
    "                return max(0.0, min(1.0, 1 - (degradation / original)))\n",
    "            \n",
    "            robustness_record.update({\n",
    "                'dri_exact_match': safe_dri(original_perf['exact_match_accuracy'], row['exact_match_accuracy']),\n",
    "                'dri_f1': safe_dri(original_perf['partial_match_f1'], row['partial_match_f1']),\n",
    "                'dri_value_accuracy': safe_dri(original_perf['value_extraction_accuracy'], row['value_extraction_accuracy']),\n",
    "                'dri_structural': safe_dri(original_perf['structural_understanding'], row['structural_understanding'])\n",
    "            })\n",
    "            \n",
    "            # Calculate composite DRI (PRIMARY METRIC)\n",
    "            dri_scores = [\n",
    "                robustness_record['dri_exact_match'],\n",
    "                robustness_record['dri_f1'],\n",
    "                robustness_record['dri_value_accuracy'],\n",
    "                robustness_record['dri_structural']\n",
    "            ]\n",
    "            robustness_record['composite_dri'] = np.mean(dri_scores)\n",
    "            \n",
    "            robustness_results.append(robustness_record)\n",
    "    \n",
    "    print(f\" Matched {matched_count} perturbations to original charts\")\n",
    "    print(f\" Created {len(robustness_results)} robustness comparisons\")\n",
    "    \n",
    "    if not robustness_results:\n",
    "        print(\" No robustness comparisons created - check ID matching logic\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    robustness_df = pd.DataFrame(robustness_results)\n",
    "    \n",
    "    # Save robustness analysis\n",
    "    robustness_df.to_csv('data/analysis_cache/robustness_analysis.csv', index=False)\n",
    "    print(f\" Robustness analysis saved with {len(robustness_df)} records\")\n",
    "    \n",
    "    return robustness_df\n",
    "# Calculate robustness metrics\n",
    "robustness_df = calculate_robustness_metrics_fixed(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42017677",
   "metadata": {},
   "source": [
    "### SECTION 6: SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74547750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 6: SUMMARY STATISTICS\n",
      " PERFORMANCE SUMMARY:\n",
      "------------------------------------------------------------\n",
      " ORIGINAL CHART PERFORMANCE:\n",
      "   exact_match_accuracy: 0.000 ¬± 0.000\n",
      "   partial_match_f1: 0.114 ¬± 0.191\n",
      "   value_extraction_accuracy: 8.486 ¬± 17.430\n",
      "   structural_understanding: 63.001 ¬± 37.434\n",
      "\n",
      " PERTURBATION PERFORMANCE:\n",
      "   exact_match_accuracy: 0.000 ¬± 0.000\n",
      "   partial_match_f1: 0.112 ¬± 0.192\n",
      "   value_extraction_accuracy: 8.249 ¬± 16.667\n",
      "   structural_understanding: 61.138 ¬± 36.509\n",
      "\n",
      " ROBUSTNESS ANALYSIS:\n",
      "   Mean Composite DRI: 0.883\n",
      "   DRI Standard Deviation: 0.195\n",
      "   Best DRI Score: 1.000\n",
      "   Worst DRI Score: 0.250\n",
      "\n",
      " PERTURBATION TYPE ANALYSIS:\n",
      "   blocks: DRI = 0.776 ¬± 0.247 (n=84.0)\n",
      "   blur: DRI = 0.895 ¬± 0.188 (n=180.0)\n",
      "   corruption: DRI = 0.879 ¬± 0.188 (n=83.0)\n",
      "   rotation: DRI = 0.896 ¬± 0.179 (n=167.0)\n",
      "   shift: DRI = 0.910 ¬± 0.179 (n=184.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 6: SUMMARY STATISTICS\")\n",
    "\n",
    "def generate_summary_statistics(metrics_df, robustness_df):\n",
    "    \"\"\"Generate comprehensive summary statistics\"\"\"\n",
    "    \n",
    "    print(\" PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Original performance\n",
    "    original_data = metrics_df[metrics_df['extraction_type'] == 'original']\n",
    "    \n",
    "    if not original_data.empty:\n",
    "        print(\" ORIGINAL CHART PERFORMANCE:\")\n",
    "        for metric in ['exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding']:\n",
    "            mean_val = original_data[metric].mean()\n",
    "            std_val = original_data[metric].std()\n",
    "            print(f\"   {metric}: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
    "    \n",
    "    # Perturbation performance\n",
    "    perturbation_data = metrics_df[metrics_df['extraction_type'] == 'perturbation']\n",
    "    \n",
    "    if not perturbation_data.empty:\n",
    "        print(f\"\\n PERTURBATION PERFORMANCE:\")\n",
    "        for metric in ['exact_match_accuracy', 'partial_match_f1', 'value_extraction_accuracy', 'structural_understanding']:\n",
    "            mean_val = perturbation_data[metric].mean()\n",
    "            std_val = perturbation_data[metric].std()\n",
    "            print(f\"   {metric}: {mean_val:.3f} ¬± {std_val:.3f}\")\n",
    "    \n",
    "    # Robustness summary\n",
    "    if not robustness_df.empty:\n",
    "        print(f\"\\n ROBUSTNESS ANALYSIS:\")\n",
    "        print(f\"   Mean Composite DRI: {robustness_df['composite_dri'].mean():.3f}\")\n",
    "        print(f\"   DRI Standard Deviation: {robustness_df['composite_dri'].std():.3f}\")\n",
    "        print(f\"   Best DRI Score: {robustness_df['composite_dri'].max():.3f}\")\n",
    "        print(f\"   Worst DRI Score: {robustness_df['composite_dri'].min():.3f}\")\n",
    "        \n",
    "        # Perturbation type analysis\n",
    "        print(f\"\\n PERTURBATION TYPE ANALYSIS:\")\n",
    "        perturbation_summary = robustness_df.groupby('perturbation_type')['composite_dri'].agg(['mean', 'std', 'count'])\n",
    "        for pert_type, stats in perturbation_summary.iterrows():\n",
    "            print(f\"   {pert_type}: DRI = {stats['mean']:.3f} ¬± {stats['std']:.3f} (n={stats['count']})\")\n",
    "\n",
    "generate_summary_statistics(metrics_df, robustness_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c93c8b",
   "metadata": {},
   "source": [
    "#### SECTION 7: ANALYSIS COMPLETION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e029a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SECTION 7: ANALYSIS COMPLETION\n",
      " Analysis summary saved\n",
      " Multi-metric analysis complete!\n",
      " 898 evaluations processed\n",
      " Primary metric: composite_dri (Composite DRI)\n",
      " Academic metrics: 6\n",
      "\n",
      "================================================================================\n",
      " MULTI-METRIC ANALYSIS COMPLETE!\n",
      " Standard Academic Metrics Successfully Calculated\n",
      " Ready for Statistical Analysis Phase\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SECTION 7: ANALYSIS COMPLETION\")\n",
    "\n",
    "# Create final analysis summary\n",
    "analysis_summary = {\n",
    "    'analysis_timestamp': datetime.now().isoformat(),\n",
    "    'total_evaluations': len(metrics_df),\n",
    "    'original_evaluations': len(metrics_df[metrics_df['extraction_type'] == 'original']),\n",
    "    'perturbation_evaluations': len(metrics_df[metrics_df['extraction_type'] == 'perturbation']),\n",
    "    'robustness_comparisons': len(robustness_df),\n",
    "    'primary_metric': 'composite_dri',\n",
    "    'metrics_calculated': list(STANDARD_METRICS.keys()),\n",
    "    'academic_references': [info['reference'] for info in STANDARD_METRICS.values()],\n",
    "    'data_files_created': [\n",
    "        'data/analysis_cache/comprehensive_metrics.csv',\n",
    "        'data/analysis_cache/robustness_analysis.csv'\n",
    "    ],\n",
    "    'ready_for_statistical_analysis': True,\n",
    "    'next_notebook': '06_Statistical_Analysis.ipynb'\n",
    "}\n",
    "\n",
    "# Save analysis summary\n",
    "with open('data/analysis_cache/analysis_summary.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2)\n",
    "\n",
    "print(\" Analysis summary saved\")\n",
    "print(f\" Multi-metric analysis complete!\")\n",
    "print(f\" {analysis_summary['total_evaluations']} evaluations processed\")\n",
    "print(f\" Primary metric: {analysis_summary['primary_metric']} (Composite DRI)\")\n",
    "print(f\" Academic metrics: {len(analysis_summary['metrics_calculated'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" MULTI-METRIC ANALYSIS COMPLETE!\")\n",
    "print(\" Standard Academic Metrics Successfully Calculated\")\n",
    "print(\" Ready for Statistical Analysis Phase\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Log completion\n",
    "logger.info(\"Multi-metric analysis completed successfully\")\n",
    "logger.info(f\"Total evaluations: {analysis_summary['total_evaluations']}\")\n",
    "logger.info(f\"Primary metric: {analysis_summary['primary_metric']}\")\n",
    "logger.info(\"Ready for statistical analysis phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad8339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Robustness analysis file found!\n",
      " Robustness comparisons: 698\n",
      " Mean Composite DRI: 0.883\n",
      "\n",
      " DRI by perturbation type:\n",
      "perturbation_type\n",
      "blocks        0.776\n",
      "corruption    0.879\n",
      "blur          0.895\n",
      "rotation      0.896\n",
      "shift         0.910\n",
      "Name: composite_dri, dtype: float64\n",
      "\n",
      " DRI Distribution:\n",
      "   Best DRI: 1.000\n",
      "   Worst DRI: 0.250\n",
      "   Std Dev: 0.195\n"
     ]
    }
   ],
   "source": [
    "# Run this AFTER Notebook 5 is completely done\n",
    "import pandas as pd\n",
    "\n",
    "# Check if robustness analysis worked\n",
    "try:\n",
    "    df = pd.read_csv('data/analysis_cache/robustness_analysis.csv')\n",
    "    print(f\" Robustness analysis file found!\")\n",
    "    print(f\" Robustness comparisons: {len(df)}\")\n",
    "    print(f\" Mean Composite DRI: {df['composite_dri'].mean():.3f}\")\n",
    "    print(f\"\\n DRI by perturbation type:\")\n",
    "    print(df.groupby('perturbation_type')['composite_dri'].mean().sort_values().round(3))\n",
    "    \n",
    "    print(f\"\\n DRI Distribution:\")\n",
    "    print(f\"   Best DRI: {df['composite_dri'].max():.3f}\")\n",
    "    print(f\"   Worst DRI: {df['composite_dri'].min():.3f}\")\n",
    "    print(f\"   Std Dev: {df['composite_dri'].std():.3f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\" Robustness analysis file not found - Notebook 5 may not have completed Section 5\")\n",
    "except Exception as e:\n",
    "    print(f\" Error reading robustness analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d45067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
